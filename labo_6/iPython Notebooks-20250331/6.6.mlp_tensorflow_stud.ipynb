{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expressed-suffering",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron (MLP) based on TensorFlow\n",
    "\n",
    "Multi-class classification problem - using a MLP with configurable number of hidden neurons - with a configurable number of classes (up to 10). It selects them from the (Fashion-)MNIST dataset, splits it up into a train and test part, does normalisation and then trains a classifier using softmax.\n",
    "\n",
    "Both datasets consist of images with 28x28 = 784 pixel each. The features refer to these pixel values of the images.\n",
    "\n",
    "You can choose MNIST or Fashion-MNIST data in cell [2]\n",
    "\n",
    "We use the Keras TensorFlow API in particular the Sequential model, which provides all required layer types [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from utils import read_data, plot_img, plot_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd9ac65-cf2d-4571-93ce-aeb172cbfbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#note the path in read_data for the data, which points to ../week1/data\n",
    "x, y, labels_map = read_data('fashionMNIST', storage_path='../week1/data') #MNIST or fashionMNIST\n",
    "x = x.numpy()\n",
    "y = y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "#append rows x cols tiles of images\n",
    "rows = 8\n",
    "cols = 18\n",
    "#figure size can be set\n",
    "fig_size = [8,8]\n",
    "\n",
    "plot_tiles(x, rows, cols, fig_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-charm",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#choose a given class 0..9\n",
    "digit  = 0\n",
    "\n",
    "plot_tiles(x[y==digit], rows, cols, fig_size)\n",
    "print(labels_map[digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e7f320-89df-4cd2-a795-1b4fcee72901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the classes for your training and test set, select train and test split and to normalization\n",
    "def prepare_data(classes, train_size=0.8, min_max_normalise=1, flatten=1):\n",
    "    \"\"\"\n",
    "    prepare the data for training\n",
    "\n",
    "    Arguments:\n",
    "    classes -- list of classes to use for training (at least two classes must be given)\n",
    "    train_size -- fraction of train image size\n",
    "    min_max_normalise -- whether to do min-max-normalisation (1) or rescaling (0)\n",
    "    flatten -- whether to flatten the 28x28 image to single row (=1); otherwise a new dimension is added at axis=1 (to be compatible with cnn)\n",
    "    \"\"\"\n",
    "\n",
    "    if len(classes) < len(labels_map):\n",
    "        for label in classes:\n",
    "            print('labels chosen are: %r' % labels_map[label])\n",
    "\n",
    "    ind_sel = np.isin(y, classes)\n",
    "    x_sel = x[ind_sel,:].copy()\n",
    "    y_sel = y[ind_sel].copy()\n",
    "\n",
    "    #replace the labels such that they are in successive order\n",
    "    for i0 in range(0,len(classes)):\n",
    "        if i0 != classes[i0]:\n",
    "            y_sel[y_sel == classes[i0]] = i0\n",
    "\n",
    "    #we give y back as simple vector -> simplifies handling below\n",
    "    #y_sel = np.reshape(y_sel, (-1,1))\n",
    "    \n",
    "    #do train and test split\n",
    "    num_samples = x_sel.shape[0]\n",
    "    max_train_ind = int(train_size*num_samples)\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    x_train = x_sel[indices[:max_train_ind]]\n",
    "    x_test = x_sel[indices[max_train_ind:]]\n",
    "    \n",
    "    y_train = y_sel[indices[:max_train_ind]]\n",
    "    y_test = y_sel[indices[max_train_ind:]]\n",
    "\n",
    "    #perform normalisation, take care of converting data type to float!\n",
    "    xmax, xmin = np.max(x_train), np.min(x_train)\n",
    "    \n",
    "    if min_max_normalise:\n",
    "        x_train = 2*(x_train.astype(float) - xmin) / (xmax - xmin) - 1\n",
    "        x_test = 2*(x_test.astype(float) - xmin) / (xmax - xmin) - 1\n",
    "    else:\n",
    "        x_train = x_train.astype(float) / xmax \n",
    "        x_test = x_test.astype(float) / xmax \n",
    "\n",
    "    if flatten:\n",
    "        m = x_train.shape[0]\n",
    "        x_train = x_train.reshape([m,-1])\n",
    "        m = x_test.shape[0]\n",
    "        x_test = x_test.reshape([m,-1])\n",
    "    else:\n",
    "        x_train = np.expand_dims(x_train,1)\n",
    "        x_test = np.expand_dims(x_test,1)\n",
    "\n",
    "    #tensorflow requires onehot data\n",
    "    y_train = one_hot(y_train, len(classes))\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def one_hot(y, size_out):\n",
    "        \"\"\"\n",
    "        construct onehot vector from set of labels\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "        one_hot = np.zeros((m, size_out), dtype=float)\n",
    "        one_hot[np.arange(m), y] = 1\n",
    "\n",
    "        return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bcd5fb-d25e-4fe5-906e-386c955898a1",
   "metadata": {},
   "source": [
    "### Class NeuralNetwork\n",
    "\n",
    "This class constructs a Multilayer Perceptron with a configurable number of hidden layers. Cost function is CE. The method $propagate()$ returns the prediction $$ \\hat{y}^{(i)}=h_\\theta(\\mathbf{x}^{(i)}) $$ on the input data (can be a n x 784 matrix of n images) and $back\\_propagate()$ determines the gradients of the cost function with respect to the parameters (weights and bias for all layers) $$ \\nabla_{\\mathbf{\\theta}} J(\\mathbf{\\theta}) $$\n",
    "The method $gradient\\_descend()$ finally does the correction of the parameters with a step in the negative gradient direction, weighted with the learning rate $$\\alpha$$ for all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c173bc6-1969-4bd7-87fe-b6807f8529d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    MLP class handling the layers and doing all propagation and back propagation steps\n",
    "    all hidden layers are dense (with ReLU activation) and the last layer is softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, list_num_neurons, alpha):\n",
    "        \"\"\"\n",
    "        constructor\n",
    "\n",
    "        Arguments:\n",
    "        list_num_neurons -- list of layer sizes including in- and output layer\n",
    "        alpha -- learning rate (required because optimiser must be known to compile model)\n",
    "        \"\"\"\n",
    "        self.model = tf.keras.Sequential()\n",
    "        \n",
    "        #we require a flatten tensor\n",
    "        self.model.add(tf.keras.layers.Flatten(input_shape=(1, 28, 28)))\n",
    "        \n",
    "        for i0 in range(len(list_num_neurons)-1):\n",
    "            self.model.add(tf.keras.layers.Dense(list_num_neurons[i0], activation='sigmoid'))\n",
    "            \n",
    "        #finally add softmax layer\n",
    "        self.model.add(tf.keras.layers.Dense(list_num_neurons[-1], activation='softmax'))\n",
    "\n",
    "        print(self.model.summary())\n",
    "                         \n",
    "        #choose the optimiser\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=alpha)\n",
    "        \n",
    "        #wrap the model to a tf.function (unless run_eagerly=True)\n",
    "        self.model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "         \n",
    "        \n",
    "    def optimise(self, data, epochs, valid_size=0.2, batch_size=16, debug=0, call_backs=None):\n",
    "        \"\"\"\n",
    "        performs epochs number of gradient descend steps and appends result to output array\n",
    "\n",
    "        Arguments:\n",
    "        data -- dictionary with NORMALISED data\n",
    "        epochs -- number of epochs\n",
    "        valid_size -- fraction of data used for validation\n",
    "        batch_size -- size of batches (1 = SGD, 1 < .. < n = mini-batch)\n",
    "        debug -- output: 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
    "        call_backs -- use to send output to tensorboard\n",
    "        \"\"\"\n",
    "\n",
    "        #now start training (you can choose the train/validation set split)\n",
    "        self.history = self.model.fit(data['x_train'], data['y_train'], \n",
    "                            validation_split=valid_size,   \n",
    "                            batch_size=batch_size, epochs=epochs,\n",
    "                            callbacks=call_backs, verbose = debug)\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b608513-6265-4dbb-8227-753da1473aeb",
   "metadata": {},
   "source": [
    "### Plot Function for Tensorflow\n",
    "\n",
    "The two functions below illustrate how to access the result data without using tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5486e4df-7483-4fee-8822-f1535cc720fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error(nn_instance):\n",
    "    \"\"\"\n",
    "    analyse error as function of epochs\n",
    "\n",
    "    Arguments:\n",
    "    nn_instance -- NeuralNetwork class to plot\n",
    "    \"\"\"\n",
    "    epochs = np.arange(len(NNet.history.history['accuracy']))\n",
    "    train_acc = nn_instance.history.history['accuracy']\n",
    "    val_acc = nn_instance.history.history['val_accuracy']\n",
    "\n",
    "    plt.plot(epochs, train_acc, label=\"train\")\n",
    "    plt.plot(epochs, val_acc, label=\"validation\")\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    xmax = epochs[-1]\n",
    "    ymin = min(min(train_acc),min(val_acc))\n",
    "    ymax = 1\n",
    "    plt.axis([0,xmax,ymin,ymax])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_cost(nn_instance):\n",
    "    \"\"\"\n",
    "    analyse cost as function of epochs\n",
    "\n",
    "    Arguments:\n",
    "    nn_instance -- NeuralNetwork class to plot\n",
    "    \"\"\"\n",
    "    epochs = np.arange(len(nn_instance.history.history['loss']))\n",
    "    train_costs = nn_instance.history.history['loss']\n",
    "    val_costs = nn_instance.history.history['val_loss']\n",
    "\n",
    "    plt.semilogy(epochs, train_costs, label=\"train\")\n",
    "    plt.semilogy(epochs, val_costs, label=\"validation\")\n",
    "    plt.ylabel('Cost')\n",
    "    plt.xlabel('Epochs')\n",
    "    xmax = epochs[-1]\n",
    "    ymin = 1e-2\n",
    "    ymax = 2\n",
    "    plt.axis([0,xmax,ymin,ymax])\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcfae34-9c4a-461f-b06f-d9abc093b775",
   "metadata": {},
   "source": [
    "### Sample execution of Neural Network\n",
    "\n",
    "The cell below shows how to use the class NeuralNetwork and how to perform the optimisation. The training and test data is given as dictionary in the call to the method $optimise()$. The classes (from 2 to 10) can be chosen via the `classes` list. \n",
    "\n",
    "*This version is used without TensorBoard*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5e5245-456f-4026-8978-8a1b8776ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose the categories\n",
    "classes = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "#y_train is of type onehot (y_test not!)\n",
    "x_train, x_test, y_train, y_test = prepare_data(classes, train_size=0.8, min_max_normalise=1, flatten=0)\n",
    "\n",
    "#data is arranged as dictionary with quick access through respective keys\n",
    "#tensorflow does the split in train and validate\n",
    "data = {'x_train' : x_train, 'y_train' : y_train}\n",
    "\n",
    "#choose the hyperparameters you want to use for the initialisation\n",
    "size_out = len(classes)\n",
    "list_num_neurons = [100, size_out]; \n",
    "learning_rate = 0.05\n",
    "NNet = NeuralNetwork(list_num_neurons, learning_rate)\n",
    "\n",
    "#choose the hyperparameters you want to use for training\n",
    "epochs = 100\n",
    "batchsize = 16\n",
    "NNet.optimise(data, epochs, valid_size=0.2, batch_size=batchsize, debug=2)\n",
    "\n",
    "plot_error(NNet)\n",
    "plot_cost(NNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b7882-711e-4aea-8e93-6aedd179c8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse false classified training or test images\n",
    "y_pred = np.argmax(NNet.model.predict(x_test), axis=1)\n",
    "false_classifications = x_test[(y_pred != y_test)]\n",
    "\n",
    "print('test error rate: %.2f %% out of %d' % (100*false_classifications.shape[0]/y_pred.shape[0], y_pred.shape[0]))\n",
    "print(false_classifications.shape)\n",
    "\n",
    "#append rows x cols tiles of digits\n",
    "rows = 7\n",
    "cols = 8\n",
    "#figure size can be set\n",
    "fig_size = [8,8]\n",
    "\n",
    "plot_tiles(false_classifications.reshape([-1,28,28]), rows, cols, fig_size)\n",
    "\n",
    "#print the correct labels (for FashionMNIST)\n",
    "if rows*cols < false_classifications.shape[0]:\n",
    "    false_classifications_y = y_test[y_pred != y_test][:rows*cols]\n",
    "else:\n",
    "    false_classifications_y = np.append(y_test[y_pred != y_test], np.ones(rows*cols - false_classifications.shape[0])*-1)\n",
    "print(false_classifications_y.reshape([cols,rows]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee00a7a-f1d3-45df-8ea9-b52abdf583c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#visualise weights of the first layer\n",
    "\n",
    "print('we have %r weight vectors in layer [0]' % NNet.model.layers[1].get_weights()[0].shape[1])\n",
    "print('choose a suitable combination of rows and cols below to plot them')\n",
    "\n",
    "rows = 5\n",
    "cols = 20\n",
    "#figure size can be set\n",
    "fig_size = [14,6]\n",
    "\n",
    "plot_tiles(NNet.model.layers[1].get_weights()[0].T.reshape([-1,28,28]), rows, cols, fig_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e89671d-0e9a-4c47-93eb-c820e98cbd68",
   "metadata": {},
   "source": [
    "### Sample execution of Neural Network\n",
    "\n",
    "The cell below shows how to use the class NeuralNetwork and how to perform the optimisation. The training and test data is given as dictionary in the call to the method $optimise()$. The classes (from 2 to 10) can be chosen via the `classes` list. \n",
    "\n",
    "*This version is used with TensorBoard*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b277ae9-5cca-42c8-af75-77687fe4065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear everything\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "#you may have to delete the model to start from scratch\n",
    "#del model\n",
    "\n",
    "#output is written to the folder with name defined below\n",
    "name = 'fashion_mnist_experiment_tf'\n",
    "\n",
    "tensorboard = TensorBoard(\n",
    "        log_dir='./tensorboard/' + name + '/', \n",
    "        update_freq='epoch', write_graph=True,\n",
    "        histogram_freq=1)\n",
    "\n",
    "\n",
    "#choose the categories\n",
    "classes = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "#y_train is of type onehot (y_test not!)\n",
    "x_train, x_test, y_train, y_test = prepare_data(classes, train_size=0.8, min_max_normalise=1, flatten=0)\n",
    "\n",
    "#data is arranged as dictionary with quick access through respective keys\n",
    "#tensorflow does the split in train and validate\n",
    "data = {'x_train' : x_train, 'y_train' : y_train}\n",
    "\n",
    "#choose the hyperparameters you want to use for the initialisation\n",
    "size_out = len(classes)\n",
    "list_num_neurons = [100, size_out]; \n",
    "learning_rate = 0.05\n",
    "NNet = NeuralNetwork(list_num_neurons, learning_rate)\n",
    "\n",
    "#choose the hyperparameters you want to use for training\n",
    "epochs = 100\n",
    "batchsize = 16\n",
    "NNet.optimise(data, epochs, valid_size=0.2, batch_size=batchsize, debug=0, call_backs=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d45038-9ba6-424f-9a51-0ff1e03a6539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

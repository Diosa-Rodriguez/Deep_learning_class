{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expressed-suffering",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron (MLP) based on Pytorch (1)\n",
    "\n",
    "Multi-class classification problem - using a MLP with configurable number of hidden neurons - with a configurable number of classes (up to 10). It selects them from the (Fashion-)MNIST dataset, splits it up into a train and test part, does normalisation and then trains a classifier using softmax.\n",
    "\n",
    "Both datasets consist of images with 28x28 = 784 pixel each. The features refer to these pixel values of the images.\n",
    "\n",
    "You can choose MNIST or Fashion-MNIST data in cell [2]\n",
    "\n",
    "We use the PyTorch nn-library providing all required layer types and in particular the Sequential Container to set up a MLP [torch.nn](https://pytorch.org/docs/stable/nn.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from utils import read_data, plot_img, plot_tiles, plot_error, plot_cost, plot_parameter_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd9ac65-cf2d-4571-93ce-aeb172cbfbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#note the path in read_data for the data, which points to ../week1/data\n",
    "x, y, labels_map = read_data('fashionMNIST', storage_path='../week1/data') #MNIST or fashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "#append rows x cols tiles of images\n",
    "rows = 8\n",
    "cols = 18\n",
    "#figure size can be set\n",
    "fig_size = [8,8]\n",
    "\n",
    "plot_tiles(x, rows, cols, fig_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-charm",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#choose a given class 0..9\n",
    "digit  = 0\n",
    "\n",
    "plot_tiles(x[y==digit], rows, cols, fig_size)\n",
    "print(labels_map[digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e7f320-89df-4cd2-a795-1b4fcee72901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the classes for your training and test set, select train and test split and to normalization\n",
    "def prepare_data(classes, train_size=0.8, min_max_normalise=1, flatten=0):\n",
    "    \"\"\"\n",
    "    prepare the data for training\n",
    "\n",
    "    Arguments:\n",
    "    classes -- list of classes to use for training (at least two classes must be given)\n",
    "    train_size -- fraction of train image size\n",
    "    min_max_normalise -- whether to do min-max-normalisation (1) or rescaling (0)\n",
    "    flatten -- whether to flatten the 28x28 image to single row (=1); otherwise a new dimension is added at axis=1 (to be compatible with cnn)\n",
    "    \"\"\"\n",
    "\n",
    "    if len(classes) < len(labels_map):\n",
    "        for label in classes:\n",
    "            print('labels chosen are: %r' % labels_map[label.item()])\n",
    "\n",
    "    ind_sel = torch.isin(y, classes)\n",
    "\n",
    "    x_sel = torch.zeros(x[ind_sel,:].shape, dtype=torch.float)\n",
    "    x_sel.copy_(x[ind_sel,:])\n",
    "    y_sel = torch.zeros(y[ind_sel].shape, dtype=y.dtype)\n",
    "    y_sel.copy_(y[ind_sel])\n",
    "\n",
    "    #replace the labels such that they are in successive order\n",
    "    for i0 in range(0,len(classes)):\n",
    "        if i0 != classes[i0]:\n",
    "            y_sel[y_sel == classes[i0]] = i0\n",
    "\n",
    "    #we give y back as simple vector -> simplifies handling below\n",
    "    #y_sel = np.reshape(y_sel, (-1,1))\n",
    "    \n",
    "    #do train and test split\n",
    "    num_samples = x_sel.shape[0]\n",
    "    max_train_ind = int(train_size*num_samples)\n",
    "    indices = torch.randperm(num_samples)\n",
    "    \n",
    "    x_train = x_sel[indices[:max_train_ind]]\n",
    "    x_test = x_sel[indices[max_train_ind:]]\n",
    "    \n",
    "    y_train = y_sel[indices[:max_train_ind]]\n",
    "    y_test = y_sel[indices[max_train_ind:]]\n",
    "\n",
    "    #perform normalisation, take care of converting data type to float!\n",
    "    xmax, xmin = torch.max(x_train), torch.min(x_train)\n",
    "    \n",
    "    if min_max_normalise:\n",
    "        x_train = 2*(x_train - xmin) / (xmax - xmin) - 1\n",
    "        x_test = 2*(x_test - xmin) / (xmax - xmin) - 1\n",
    "    else:\n",
    "        x_train = x_train / xmax \n",
    "        x_test = x_test / xmax \n",
    "\n",
    "    if flatten:\n",
    "        m = x_train.shape[0]\n",
    "        x_train = x_train.reshape([m,-1])\n",
    "        m = x_test.shape[0]\n",
    "        x_test = x_test.reshape([m,-1])\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cce1b9-2a6a-47dd-b93b-8fb8b527fd08",
   "metadata": {},
   "source": [
    "### Class MiniBatches\n",
    "\n",
    "Splits the given dataset (`x: features` and `y: labels`) into individual batches of size `batch_size` (a value of `0` will return the full batch). The total number of batches available in an epoch is returned with method `number_of_batches()`. Each call to `next()` will return a new batch in the given format: `{'x_batch': x_batch, 'y_batch': y_batch}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c698c3d-05aa-4662-b8e6-1c9065793bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatches:\n",
    "    \"\"\"\n",
    "    obtains x- and y-data in the constructor and returns a sample of batch_size with each call to next()\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y, batch_size):\n",
    "        \"\"\"\n",
    "        constructor\n",
    "\n",
    "        Arguments:\n",
    "        x/y -- data\n",
    "        batch_size -- size of batch (0 means one single batch)\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        m = x.shape[0]\n",
    "        self.indices = torch.randperm(m)\n",
    "        self.n = x.shape[1]\n",
    "        \n",
    "        if not batch_size:\n",
    "            self.batch_size = m\n",
    "            self.mb = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size        \n",
    "            self.mb = int(m / self.batch_size)    \n",
    "        \n",
    "        self.ib = 0\n",
    "\n",
    "    def number_of_batches(self):\n",
    "        return self.mb\n",
    "\n",
    "    def next(self):\n",
    "        it = self.indices[self.ib * self.batch_size:(self.ib + 1) * self.batch_size]\n",
    "        x_batch = self.x[it, :]\n",
    "        y_batch = self.y[it]\n",
    "        self.ib += 1\n",
    "\n",
    "        return {'x_batch': x_batch, 'y_batch': y_batch}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bcd5fb-d25e-4fe5-906e-386c955898a1",
   "metadata": {},
   "source": [
    "### Class NeuralNetwork\n",
    "\n",
    "This class constructs a Multilayer Perceptron with a configurable number of hidden layers. Cost function is CE. The method $propagate()$ returns the prediction $$ \\hat{y}^{(i)}=h_\\theta(\\mathbf{x}^{(i)}) $$ on the input data (can be a n x 784 matrix of n images) and $back\\_propagate()$ determines the gradients of the cost function with respect to the parameters (weights and bias for all layers) $$ \\nabla_{\\mathbf{\\theta}} J(\\mathbf{\\theta}) $$\n",
    "The method $gradient\\_descend()$ finally does the correction of the parameters with a step in the negative gradient direction, weighted with the learning rate $$\\alpha$$ for all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c173bc6-1969-4bd7-87fe-b6807f8529d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    MLP class handling the layers and doing all propagation and back propagation steps\n",
    "    all hidden layers are dense (with ReLU activation) and the last layer is softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, list_num_neurons):\n",
    "        \"\"\"\n",
    "        constructor\n",
    "\n",
    "        Arguments:\n",
    "        list_num_neurons -- list of layer sizes including in- and output layer\n",
    "        \n",
    "        \"\"\"\n",
    "        self.model = torch.nn.Sequential()\n",
    "        #first construct dense layers\n",
    "        for i0 in range(len(list_num_neurons)-2):\n",
    "            self.model.add_module('dense' + str(i0), torch.nn.Linear(list_num_neurons[i0], list_num_neurons[i0+1]))\n",
    "            self.model.add_module('act' + str(i0), torch.nn.Sigmoid())\n",
    "            \n",
    "        #finally add softmax layer\n",
    "        #we don't require activation function because it is included (for numerical reasons) in the cross \n",
    "        #entropy cost below; alternative is logSoftmax together with NLLLoss cost function\n",
    "        self.model.add_module('dense' + str(i0+1), torch.nn.Linear(list_num_neurons[-2], list_num_neurons[-1]))\n",
    "                         \n",
    "        #define the cost function\n",
    "        self.cost_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "        #used to save results\n",
    "        self.result_data = torch.tensor([])\n",
    "        \n",
    "        #we keep a global step counter, thus that optimise can be called \n",
    "        #several times with different settings\n",
    "        self.epoch_counter = 0 \n",
    "        \n",
    "    def propagate(self, x):\n",
    "        \"\"\"\n",
    "        calculates the function estimation based on current parameters\n",
    "        \"\"\"    \n",
    "        y_pred = self.model(x)\n",
    "\n",
    "        return y_pred\n",
    "           \n",
    "     \n",
    "    def back_propagate(self, cost):\n",
    "        \"\"\"\n",
    "        calculates the backpropagation results based on expected output y\n",
    "        this function must be performed AFTER the corresponding propagte step\n",
    "        \"\"\"    \n",
    "        #set gradient values to zero\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    " \n",
    "\n",
    "    def cost_funct(self, y_pred, y):\n",
    "        \"\"\"\n",
    "        calculates the MSE loss function\n",
    "        \"\"\"\n",
    "        cost = self.cost_fn(y_pred, y)\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "         \n",
    "    def gradient_descend(self, alpha):\n",
    "        \"\"\"\n",
    "        does the gradient descend based on results from last back_prop step with learning rate alpha\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            for param in self.model.parameters():\n",
    "                param -= alpha * param.grad\n",
    "            \n",
    "         \n",
    "    def calc_error(self, y_pred, y):\n",
    "        \"\"\"\n",
    "        get error information\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "\n",
    "        y_pred_argmax = torch.argmax(y_pred, dim=1)\n",
    "        error = torch.sum(y != y_pred_argmax) / m\n",
    "\n",
    "        return error\n",
    "\n",
    "    \n",
    "    def append_result(self):\n",
    "        \"\"\"\n",
    "        append cost and error data to output array\n",
    "        \"\"\"\n",
    "        # determine cost and error functions for train and validation data\n",
    "        y_pred_train = self.propagate(self.data['x_train'])\n",
    "        y_pred_val = self.propagate(self.data['x_val'])\n",
    "\n",
    "        res_data = torch.tensor([[self.cost_funct(y_pred_train, self.data['y_train']), \n",
    "                                  self.calc_error(y_pred_train, self.data['y_train']),\n",
    "                                  self.cost_funct(y_pred_val, self.data['y_val']), \n",
    "                                  self.calc_error(y_pred_val, self.data['y_val'])]])\n",
    "        \n",
    "        self.result_data = torch.cat((self.result_data, res_data), 0)\n",
    "\n",
    "        #increase epoch counter here (used for plot routines below)\n",
    "        self.epoch_counter += 1 \n",
    "        \n",
    "        return res_data\n",
    "\n",
    "        \n",
    "    def optimise(self, data, epochs, alpha, batch_size=0, debug=0):\n",
    "        \"\"\"\n",
    "        performs epochs number of gradient descend steps and appends result to output array\n",
    "\n",
    "        Arguments:\n",
    "        data -- dictionary with NORMALISED data\n",
    "        epochs -- number of epochs\n",
    "        alpha -- learning rate\n",
    "        batch_size -- size of batches (1 = SGD, 0 = batch, 1 < .. < n = mini-batch)\n",
    "        debug -- integer value; get info on gradient descend step every debug-step (0 -> no output)\n",
    "        \"\"\"\n",
    "        #access to data from other methods\n",
    "        self.data = data\n",
    "        \n",
    "        # save results before 1st step\n",
    "        if self.epoch_counter == 0:\n",
    "            res_data = self.append_result()\n",
    "\n",
    "        for i0 in range(0, epochs):    \n",
    "            #measure time for one epoch\n",
    "            start=time.time()\n",
    "            # create batches for each epoch\n",
    "            batches = MiniBatches(self.data['x_train'], self.data['y_train'], batch_size)\n",
    "            #set model to training mode\n",
    "            self.model.train()\n",
    "            for ib in range(batches.number_of_batches()):\n",
    "                batch = batches.next()\n",
    "                #do prediction\n",
    "                y_pred = self.propagate(batch['x_batch'])\n",
    "                #determine the loss \n",
    "                cost = self.cost_funct(y_pred, batch['y_batch'])\n",
    "                #determine the error\n",
    "                self.back_propagate(cost)\n",
    "                #do the correction step\n",
    "                self.gradient_descend(alpha)\n",
    "\n",
    "            #save result\n",
    "            self.model.eval()\n",
    "            res_data = self.append_result()\n",
    "\n",
    "            #end of time measurement\n",
    "            end=time.time()\n",
    "            \n",
    "            if debug and np.mod(i0, debug) == 0:\n",
    "                print('result after %d epochs (dt=%1.2f s), train: cost %.5f, error %.5f ; validation: cost %.5f, error %.5f'\n",
    "                    % (self.epoch_counter-1, end-start, res_data[0, 0].item(), res_data[0, 1].item(), \\\n",
    "                                                                res_data[0, 2].item(), res_data[0, 3].item()))\n",
    "\n",
    "        if debug:\n",
    "            print('result after %d epochs, train: cost %.5f, error %.5f ; validation: cost %.5f, error %.5f'\n",
    "                  % (self.epoch_counter-1, res_data[0, 0].item(), res_data[0, 1].item(), \\\n",
    "                                                                res_data[0, 2].item(), res_data[0, 3].item()))\n",
    "                        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcfae34-9c4a-461f-b06f-d9abc093b775",
   "metadata": {},
   "source": [
    "### Sample execution of Neural Network\n",
    "\n",
    "The cell below shows how to use the class NeuralNetwork and how to perform the optimisation. The training and test data is given as dictionary in the call to the method $optimise()$. The classes (from 2 to 10) can be chosen via the `classes` list. This method can be called several times in a row with different arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5e5245-456f-4026-8978-8a1b8776ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose the categories\n",
    "classes = torch.tensor([0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "x_train, x_test, y_train, y_test = prepare_data(classes, train_size=0.8, min_max_normalise=1, flatten=1)\n",
    "\n",
    "#further split in train and validation data\n",
    "validation_size = 0.2\n",
    "valid_ind = int(x_train.shape[0]*(1-validation_size))\n",
    "\n",
    "#data is arranged as dictionary with quick access through respective keys\n",
    "data = {'x_train' : x_train[:valid_ind,:], 'y_train' : y_train[:valid_ind],  \\\n",
    "               'x_val' : x_train[valid_ind:,:], 'y_val' : y_train[valid_ind:]}\n",
    "\n",
    "#choose the hyperparameters you want to use for the initialisation\n",
    "size_in = x_train.shape[1]\n",
    "size_out = len(classes)\n",
    "list_num_neurons = [size_in, 100, size_out]; \n",
    "NNet = NeuralNetwork(list_num_neurons)\n",
    "\n",
    "#choose the hyperparameters you want to use for training\n",
    "epochs = 40\n",
    "batchsize = 16\n",
    "learning_rate = 0.05\n",
    "NNet.optimise(data, epochs, learning_rate, batchsize, debug=5)\n",
    "\n",
    "\n",
    "plot_error(NNet)\n",
    "plot_cost(NNet)\n",
    "\n",
    "plot_parameter_hist(NNet)\n",
    "\n",
    "y_pred = torch.argmax(NNet.propagate(x_test), axis=1)\n",
    "false_classifications = x_test[(y_pred != y_test)]\n",
    "\n",
    "print('test error rate: %.2f %% out of %d' % (100*false_classifications.shape[0]/y_pred.shape[0], y_pred.shape[0]))\n",
    "print(false_classifications.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b7882-711e-4aea-8e93-6aedd179c8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse false classified training or test images\n",
    "y_pred = torch.argmax(NNet.propagate(x_test), axis=1)\n",
    "false_classifications = x_test[(y_pred != y_test)]\n",
    "\n",
    "print('test error rate: %.2f %% out of %d' % (100*false_classifications.shape[0]/y_pred.shape[0], y_pred.shape[0]))\n",
    "print(false_classifications.shape)\n",
    "\n",
    "#append rows x cols tiles of digits\n",
    "rows = 7\n",
    "cols = 8\n",
    "#figure size can be set\n",
    "fig_size = [8,8]\n",
    "\n",
    "plot_tiles(false_classifications.reshape([-1,28,28]), rows, cols, fig_size)\n",
    "\n",
    "#print the correct labels (for FashionMNIST)\n",
    "if rows*cols < false_classifications.shape[0]:\n",
    "    false_classifications_y = y_test[y_pred != y_test][:rows*cols]\n",
    "else:\n",
    "    false_classifications_y = np.append(y_test[y_pred != y_test], np.ones(rows*cols - false_classifications.shape[0])*-1)\n",
    "print(false_classifications_y.reshape([cols,rows]).T.to(torch.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee00a7a-f1d3-45df-8ea9-b52abdf583c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#visualise weights of the first layer\n",
    "\n",
    "print('we have %r weight vectors in layer [0]' % NNet.model[0].weight.shape[1])\n",
    "print('choose a suitable combination of rows and cols below to plot them')\n",
    "\n",
    "rows = 5\n",
    "cols = 20\n",
    "#figure size can be set\n",
    "fig_size = [14,6]\n",
    "\n",
    "plot_tiles(NNet.model[0].weight.detach().reshape([-1,28,28]), rows, cols, fig_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfd7510-c14a-4069-84b1-359e3269ab8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

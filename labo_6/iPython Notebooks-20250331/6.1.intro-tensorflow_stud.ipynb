{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: \"Introduction\" to Tensorflow\n",
    "\n",
    "*TensorFlow is a free and open-source software library for machine learning and artificial intelligence. It can be used across a range of tasks but has a particular focus on training and inference of deep neural networks. It was developed by the Google Brain team for Google's internal use in research and production. The initial version was released under the Apache License 2.0 in 2015. Google released an updated version, TensorFlow 2.0, in September 2019.* [Wikipedia](https://en.wikipedia.org/wiki/TensorFlow).\n",
    "\n",
    "This file is only intended to give an intuition how the low level programming in TensorFlow works. We reproduce the sample given in the script `5.0.intro-pytorch.ipynb` for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Tensorflow and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a simple graph\n",
    "The decorator `tf.function` compiles a function into a callable TensorFlow graph. Here the same cost function (as in `6.0.intro-pytorch.ipynb`) based on W and b is determined and its gradients calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def simple_graph(x, W, b):\n",
    "    t = tf.linalg.matmul(W,x)\n",
    "    a = t + b\n",
    "    a_square = tf.math.square(a)\n",
    "    cost = tf.reduce_sum(a_square)\n",
    "    grad_W, grad_b = tf.gradients(ys=cost, xs=[W,b])\n",
    "    return cost, grad_W, grad_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1.,2.,3.]]).T\n",
    "#declare W and b as tensors with gradient determination\n",
    "W = np.random.randn(2,3) \n",
    "b = np.random.randn(2,1) \n",
    "\n",
    "#calculate a function called 'cost'\n",
    "cost, grad_W, grad_b = simple_graph(x, W, b)\n",
    "print(cost)\n",
    "print(grad_W)\n",
    "print(grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual verification of gradient determination\n",
    "\n",
    "For comparison we determine the gradient manually. Recall that \n",
    "$$\n",
    "res = \\mathbf{a}^T \\cdot \\mathbf{a} = (\\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b})^T \\cdot (\\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b}) \n",
    "$$\n",
    "Thus it is straight forward to show that:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{W}} res = 2 \\cdot (\\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b}) \\cdot \\mathbf{x}^T = 2\\cdot \\mathbf{a} \\cdot \\mathbf{x}^T\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{b}} res = 2 \\cdot (\\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b})= 2\\cdot \\mathbf{a}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = W@x + b\n",
    "W_grad = 2*a@x.T\n",
    "b_grad = 2*a\n",
    "print(W_grad)\n",
    "print(b_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
